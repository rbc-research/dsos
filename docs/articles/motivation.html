<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>A 10-minute Crash Course • dsos</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/journal/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="A 10-minute Crash Course">
<meta property="og:description" content="dsos">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">dsos</a>
        <span class="version label label-info" data-toggle="tooltip" data-placement="bottom" title="">0.1.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/diy-score.html">Bring Your Own Scores</a>
    </li>
    <li>
      <a href="../articles/motivation.html">A 10-minute Crash Course</a>
    </li>
    <li>
      <a href="../articles/dependencies.html">Acknowledgements</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/vathymut/dsos/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>A 10-minute Crash Course</h1>
                        <h4 data-toc-skip class="author">Vathy M.
Kamulete</h4>
            <address class="author_afil">
      Royal Bank of
Canada<br><a class="author_email" href="mailto:#"></a><a href="mailto:vathy.kamulete@rbc.com" class="email">vathy.kamulete@rbc.com</a>
      </address>
                  
            <h4 data-toc-skip class="date">Last Updated: 2022-07-22</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/vathymut/dsos/blob/HEAD/vignettes/motivation.Rmd" class="external-link"><code>vignettes/motivation.Rmd</code></a></small>
      <div class="hidden name"><code>motivation.Rmd</code></div>

    </div>

    
    
<p>Note that this vignette is adapted from the <a href="https://openreview.net/forum?id=S5UG2BLi9xc" class="external-link">paper</a> as cited
below <span class="citation">(Kamulete 2022)</span>.</p>
<div class="section level2">
<h2 id="background">Background<a class="anchor" aria-label="anchor" href="#background"></a>
</h2>
<p>Suppose we fit a predictive model on a training set and predict on a
test set. Dataset shift, also known as data or population drift, occurs
when training and test distributions are not alike. This is essentially
a sample mismatch problem. Some regions of the data space are either too
sparse or absent during training and gain importance at test time. We
want methods to alert us to the presence of unexpected inputs in the
test set. To do so, a measure of divergence between training and test
set is required. Can we not simply use the many modern off-the-shelf
multivariate tests of equal distributions for this?</p>
<p>One reason for moving beyond tests of equal distributions is that
they are often too strict. They require high fidelity between training
and test set everywhere in the input domain. However, not <em>all</em>
changes in distribution are a cause for concern – some changes are
benign. Practitioners distrust these tests because of false alarms.
<span class="citation">Polyzotis et al. (2019)</span> comment:</p>
<blockquote>
<p>statistical tests for detecting changes in the data distribution […]
are too sensitive and also uninformative for the typical scale of data
in machine learning pipelines, which led us to seek alternative methods
to quantify changes between data distributions.</p>
</blockquote>
<p>Even when the difference is small or negligible, tests of equal
distributions reject the null hypothesis of no difference. An alarm
should only be raised if a shift warrants intervention. Retraining
models when distribution shifts are benign is both costly and
ineffective. Monitoring model performance and data quality is a critical
part of deploying safe and mature models in production. To tackle these
challenges, we propose <code>D-SOS</code> instead.</p>
<p>In comparing the test set to the training set, <code>D-SOS</code>
pays more attention to the regions –- typically, the outlying regions –-
where we are most vulnerable. To confront false alarms, it uses a robust
test statistic, namely the weighted area under the receiver operating
characteristic curve (WAUC). The weights in the WAUC discount the safe
regions of the distribution. To the best of our knowledge, this is the
first time that the WAUC is being used as a test statistic in this
context. The goal of <code>D-SOS</code> is to detect <em>non-negligible
adverse shifts</em>. This is reminiscent of noninferiority tests, widely
used in healthcare to determine if a new treatment is in fact not
inferior to an older one. Colloquially, the <code>D-SOS</code> null
hypothesis holds that the new sample is not substantively worse than the
old sample, and not that the two are equal.</p>
<p><code>D-SOS</code> moves beyond tests of equal distributions and lets
users specify which notions of outlyingness to probe. The choice of the
score function plays a central role in formalizing what we mean by
<em>worse</em>. The scores can come from out-of-distribution detection,
two-sample classification, uncertainty quantification, residual
diagnostics, density estimation, dimension reduction, and more. While
some of these scores are underused and underappreciated in two-sample
tests, they can be more informative in some cases. The main takeaway is
that given a method to assign an outlier score to a data point,
<code>D-SOS</code> uplifts these scores and turns them into a two-sample
test for no adverse shift.</p>
</div>
<div class="section level2">
<h2 id="motivational-example">Motivational example<a class="anchor" aria-label="anchor" href="#motivational-example"></a>
</h2>
<p>For illustration, we apply <code>D-SOS</code> to the canonical
<code>iris</code> dataset. The task is to classify the species of Iris
flowers based on <span class="math inline">\(d=4\)</span> covariates
(features) and <span class="math inline">\(n=50\)</span> observations
for each species. We show how <code>D-SOS</code> helps diagnose false
alarms. We highlight that (1) changes in distribution do not necessarily
hurt predictive performance, and (2) points in the densest regions of
the distribution can be the most difficult – unsafe – to predict.</p>
<p>We consider four tests of no adverse shift. Each test uses a
different score. For two-sample classification, this score is the
probability of belonging to the test set. For density-based
out-of-distribution (OOD) detection, the score comes from isolation
forest – this is (roughly) inversely related to the local density. For
residual diagnostics, it is the out-of-sample (out-of-bag) prediction
error from random forests. Finally, for confidence-based OOD detection
(prediction uncertainty), it is the standard error of the mean
prediction from random forests. Only the first notion of outlyingness –
two-sample classification – pertains to modern tests of equal
distributions; the others capture other meaningful notions of adverse
shifts. For all these scores, higher is <em>worse</em>: higher scores
indicate that the observation is diverging from the desired outcome or
that it does not conform to the training set.</p>
<p>For the subsequent tests, we split iris into 2/3 training and 1/3
test set. The train-test pairs correspond to two partitioning
strategies: (1) random sampling and (2) in-distribution (most dense)
examples in the test set. How do these sample splits fare with respect
to the aforementioned tests? Let <span class="math inline">\(s\)</span>
and <span class="math inline">\(p\)</span> denote <span class="math inline">\(s\)</span>−value and <span class="math inline">\(p\)</span>−value. The results are reported on the
<span class="math inline">\(s = − log2(p)\)</span> scale because it is
intuitive. An <span class="math inline">\(s\)</span>−value of k can be
interpreted as seeing k independent coin flips with the same outcome –-
all heads or all tails –- if the null is that of a fair coin. This
conveys how incompatible the data is with the null.</p>
<p>As plotted, the case with (1) random sampling in <b><span style="color:green;">green</span></b> exemplifies the type of false
alarms we want to avoid. Two-sample classification, standing in for
tests of equal distributions, is incompatible with the null of no
adverse shift (a <span class="math inline">\(s\)</span>−value of around
8). But this shift does not carry over to the other tests. Residual
diagnostics, density-based and confidence-based OOD detection are all
fairly compatible with the view that the test set is not worse. Had we
been entirely reliant on two-sample classification, we may not have
realized that this shift is essentially benign. Tests of equal
distributions alone give a narrow perspective on dataset shift.</p>
<p><img src="images/combined_dsos.png" width="100%" height="100%"></p>
<p>Moving on to the case with (2) in-distribution test set in <b><span style="color:orange;">orange</span></b>, density-based OOD detection
does not flag this sample as expected. We might be tempted to conclude
that the in-distribution observations are safe, and yet, the tests based
on residual diagnostics and confidence-based OOD detection are fairly
incompatible with this view. Some of the densest points are concentrated
in a region where the classifier does not discriminate very well: the
species <code>versicolor</code> and <code>virginica</code> overlap. That
is, the densest observations are not necessarily safe. Density-based OOD
detection glosses over this: the trouble may well come from inliers that
are difficult to predict. We get a more holistic perspective of dataset
shift because of these complementary notions of outlyingness.</p>
<p>The point of this exercise is twofold. First, we stress the limits of
tests of equal distributions when testing for dataset shift. They are
unable, by definition, to detect whether the shift is benign or not.
Second, we propose a family of tests based on outlier scores,
<code>D-SOS</code>, which offers a more holistic view of dataset shift.
<code>D-SOS</code> is flexible and can be easily extended to test for
other modern notions of outlyingness such as trust scores. We hope this
encourages more people to test for <em>adverse</em> shifts.</p>
</div>
<div class="section level2 unnumbered">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-kamulete2022test" class="csl-entry">
Kamulete, Vathy M. 2022. <span>“Test for Non-Negligible Adverse
Shifts.”</span> In <em>The 38th Conference on Uncertainty in Artificial
Intelligence</em>. <a href="https://openreview.net/forum?id=S5UG2BLi9xc" class="external-link">https://openreview.net/forum?id=S5UG2BLi9xc</a>.
</div>
<div id="ref-polyzotis2019data" class="csl-entry">
Polyzotis, Neoklis, Martin Zinkevich, Sudip Roy, Eric Breck, and Steven
Whang. 2019. <span>“Data Validation for Machine Learning.”</span>
<em>Proceedings of Machine Learning and Systems</em> 1: 334–47.
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Vathy M. Kamulete.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.6.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
